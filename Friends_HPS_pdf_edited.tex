\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
            pdftitle={Oh my God: The linguistic influence of the TV series Friends},
            colorlinks=true,
            linkcolor=blue,
            filecolor=Maroon,
            citecolor=Blue,
            urlcolor=Blue,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{longtable}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother

\title{Oh my God: The linguistic influence of the TV series Friends}
\providecommand{\subtitle}[1]{}
\subtitle{CSS Lab Holiday Paper Series 2020\footnote{The CSS Lab Holiday Paper Series is a tradition in which we publish a white paper about a fun research topic without thinking about peer-review, journal interests, grant agencies, nor anything else besides our passion for research. To learn more about the Computational Social Science Lab at the Graz University of Technology, visit \href{http://www.csslab.at}{csslab.at}}}
\author{David Garcia, Hannah Metzler, Janna Lasser, Max Pellert, Anna Di Natale\\
\emph{Graz University of Technology}}
\date{December 18th, 2020}

\begin{document}
\maketitle

Among English speakers, the phrase ``Oh my God'' might be the most frequent way to express surprise with soft blasphemy. In this first paper of the Computational Social Science Lab Holiday Paper Series, we look into how ``Oh my God'' became popular with the \href{https://www.imdb.com/title/tt0108778/}{TV series Friends} by analyzing the scripts of the series and other datsets of spoken and written expression across media and languages. We also study the demographics of who says it through Twitter data and explore how AI language models have learned to say it.

The expression ``Oh my God'' is specially prevalent in US television as part of the valleyspeak sociolect (Wikipedia \protect\hyperlink{ref-noauthor_valleyspeak_2020}{2020}) (see any episode of ``Keeping up with the Kardashians'' to have an idea). Beyond the OMG expression, valleyspeak is characterized by the frequent use of \emph{uptalk}, a speech pattern in which sentences end in a question-like tone, \emph{vocal fry}, another speech pattern in which utterances end in a vibrating sound similar to what a goat makes when queezed, and the extreme use of the word \emph{like} as a filler word. While original examples of valleyspeak originate in the San Fernando Valley in California, it can now be heard across the US and specially in mass media.

``Oh my God'' is the most characteristic reaction of the characters of Friends, which is one of the most successful sitcoms in history, even more than 15 years after it ended (Economist \protect\hyperlink{ref-TheEconomist_2019}{2019}). The \href{https://youtu.be/oqDSrMK_Gyg?t=346}{Honest Trailer of the series} points out how frequently the characters of the series react with this phrase. For example, Rachel says it several times in just \href{https://www.youtube.com/watch?v=x7pPxerXmVo}{one scene} in reaction to the sudden appearance of a pidgeon in her kitchen. The phrase is so typical of the series that it has even motivated a data science blog post about which Friends characters use it the most (Loscalzo \protect\hyperlink{ref-loscalzo_2018}{2018}). Beyond this phrase, Friends is becoming a common example for popular data science analyses, including social network analysis (Albright \protect\hyperlink{ref-Albright2015}{2015}; Sahakyan \protect\hyperlink{ref-sahakyan_one_2019}{2019}), finding the most popular character (Sohoye \protect\hyperlink{ref-sohoye_one_2019}{2019}), and of course, sentiment analysis (Bhattacharyya \protect\hyperlink{ref-bhattacharyya_sentiment_2019}{2019}).

Our aim with this article is to understand the popularity and meaning of ``Oh my God'', especially in relation with the TV series Friends. We will start by analyzing the scripts of Friends, comparing the use of ``Oh my God'' in Friends with contemporary TV shows and movies. We then continue by analyzing the expression through Google Books, inspecting what could have been the role of Friends in the use of the phrase and how it compares to similar phrases in other languages. We then analyze the current use of the phrase in social media through the analysis of a Twitter dataset, paying special attention to its use across genders and states in the US. Finally, we explore how AI language models like BERT and GPT-2 have learned the phrase and which meanings we can associate with the phrase through these models. The code, data, and detailed results of all these analyses can be found online in our \href{https://github.com/dgarcia-eu/Friends_HPS2020}{github repository}.

\newpage

\hypertarget{the-one-with-the-oh-my-god}{%
\section{The one with the ``Oh my God''}\label{the-one-with-the-oh-my-god}}

We dowloaded the scripts of all Friends episodes from this \href{https://github.com/fangj/friends}{Github repository} and processed the text, converting it to lower case and matching the regular expression \texttt{"oh{[}:punct:{]}*\textbackslash{}\textbackslash{}s*my{[}:punct:{]}*\textbackslash{}\textbackslash{}s*{[}:punct:{]}*(god)"}. This regular expression counts the instances of ``Oh my God'' with a soft rule that allows various punctuation and spaces in between words, but we do not count other variations such as ``Oh my fricking God''. We denote the count of matches of the regular expression as \emph{OMG}, defining this way the unit of our analyses. In total we found 1039 OMG the 229 episodes of the series (double episodes are merged into single files). After counting words with the \href{https://cran.r-project.org/web/packages/tm/index.html}{tm package} (Feinerer and Hornik \protect\hyperlink{ref-tm1}{2020}; Feinerer, Hornik, and Meyer \protect\hyperlink{ref-tm2}{2008}), we found that Friends has 1476.8 instances of OMG per million trigrams (i.e.~sequences of three words).

\begin{figure}

{\centering \includegraphics{Friends_HPS_pdf_files/figure-latex/episodes-1} 

}

\caption{OMG per episode in each season of Friends}\label{fig:episodes}
\end{figure}

Figure \ref{fig:episodes} shows the OMG per episode in each season of the series, There is a tendency to more OMG over the lifetime of the series, from less than 3 OMG per episode in the first season to more than 6 in the last one. To compare Friends to contemporary TV shows and movies we applied the same analysis to the 2020 edition of the Corpus Of Contemporary American English (COCA) (Davies \protect\hyperlink{ref-davies2010corpus}{2010}). In our \href{https://github.com/dgarcia-eu/Friends_HPS2020}{github repository} we share the final yearly counts of our analyses, as we are not allowed to share the raw text of the corpus by the terms to access it.

\begin{figure}

{\centering \includegraphics{Friends_HPS_pdf_files/figure-latex/tvm-1} 

}

\caption{OMG per million trigrams in TV and Movie subtitles of the COCA corpus.}\label{fig:tvm}
\end{figure}

\newpage

Figure \ref{fig:tvm} shows the yearly frequency of OMG per million trigrams in the TV and movie subtitles part of the COCA corpus. While it is clear that the phrase is very popular in US entertainment and its popularity is increasing, Friends had many more OMG than contemporary TV and movies. Friends had approximately 1477 OMG per million trigrams, which is 4.26 times what you would find on the typical TV shows and movies between 1994 and 2004 (300-400 OMG per million).

\begin{figure}

{\centering \includegraphics{Friends_HPS_pdf_files/figure-latex/spoken-1} 

}

\caption{OMG per million trigrams in spoken transcripts of the COCA corpus.}\label{fig:spoken}
\end{figure}

Figure \ref{fig:spoken} shows the yearly frequency of OMG per million trigrams in the transcripts of unscripted spoken TV shows of the COCA corpus. While the frequency is about 8.45 times higher in TV and movie subtitles than in these spoken transcripts, the increasing tendency is present too. Although the source of both corpora is TV, the spoken transcripts come from talk shows and other kinds of unscripted shows. Mass media scripts seem to use ``Oh my God'' as a way to emphasize and elicit surprise reactions in the audience, which does not happen so naturally in live unscripted television.

\newpage

\hypertarget{the-one-with-all-those-books}{%
\section{The one with all those books}\label{the-one-with-all-those-books}}

The frequency of OMG in both scripted and unscripted spoken language in TV shows and movies has been steadily increasing since the 1990s, but to test if Friends might have affected the tendency to use the phrase, we need to look at a longer time period. Inspiried by the trend previously observed in (Loscalzo \protect\hyperlink{ref-loscalzo_2018}{2018}), we study the frequency of OMG in Google Books, one of the most comprehensive records of human written communication over several centuries and languages (Michel et al. \protect\hyperlink{ref-michel2011quantitative}{2011})\footnote{We would say that our Google Books analysis an example of \emph{culturomics}, but the term seems to be used nowadays more often to talk about gut bacteria than about culture product analysis}. We use the \href{https://github.com/seancarmody/ngramr}{ngramr R package} (Carmody \protect\hyperlink{ref-ngramr}{2020}) to query the 2019 dataset of English fiction books to avoid known problems with non-fiction texts (Pechenick, Danforth, and Dodds \protect\hyperlink{ref-pechenick2015characterizing}{2015}). We also only analyze frequencies since 1900 to avoid Optical Character Recognition errors like mistaking a \href{https://books.google.com/ngrams/graph?content=fuck\&year_start=1500\&year_end=2019}{long \emph{s} for an \emph{f} in 1600s and 1700s texts.}

\begin{figure}

{\centering \includegraphics{Friends_HPS_pdf_files/figure-latex/books1-1} 

}

\caption{OMG per episode in Friends each season and yearly OMG per million trigrams in Google Books.}\label{fig:books1}
\end{figure}

Figure \ref{fig:books1} shows the frequency of OMG per million trigrams in Google Books with the number of OMG per episode of the ten seasons of friends superimposed. The frequency of OMG in books has consistently increased over more than a century. This rate apparently accelerated after Friends came out. Could Friends be responsible for additional growth in the frequency of OMG in books?

\begin{figure}

{\centering \includegraphics{Friends_HPS_pdf_files/figure-latex/books2-1} 

}

\caption{OMG per million trigrams in Google books (solid black) and time series model prediction (dashed blue). The vertical red line indicates the year of the first season.}\label{fig:books2}
\end{figure}

We tested the effect of Friends on the frequency of OMG in books with a causal inference design. First, we fit an ARIMA time series model of the log-transformed OMG frequency up to 1994, the year of the first season of Friends. We choose the model size by minimizing the BIC with the \emph{auto.arima} function of the \href{https://cran.r-project.org/web/packages/forecast/index.html}{forecast R package} (Hyndman et al. \protect\hyperlink{ref-forecast1}{2020}; Hyndman and Khandakar \protect\hyperlink{ref-forecast2}{2008}). The resulting model has an \(R^2\) of 0.92
for log-transformed values and of 0.67 for raw frequencies, with a moving average term and a positive drift term that explains the exponentially increasing shape of Figure \ref{fig:books1}.

\newpage

Figure \ref{fig:books2} shows the forecasted values of the model since 1995 versus the empirical frequency of OMG in Google Books.
From 1995, there has been an additional 57.59\% OMGs compare to what the null model predicts.
This is an impressive surplus of OMG after Friends started, but we should not fool ourselves with such causal inference arguments.
This could have been Friends or a coincidence with another influential source, for example South Park's ``oh my God, they killed Kenny''. What is clear is that Friends captured an increasing frequency of OMG and that, after the series, this frequency in books has grown even faster.


\begin{figure}

{\centering \includegraphics{Friends_HPS_pdf_files/figure-latex/booksmodel-1} 

}

\caption{Model projection of OMG frequency per trigram from 2000. Light blue lines show 90\% prediction intervals. The horizontal green line marks the 33\% frequency line. The vertical dashed red line shows where predictions start.}\label{fig:booksmodel}
\end{figure}


What does this tell us about the future of the phrase ``Oh my God''? To forecast trends, we apply the Complexity Science™ method of fitting straight lines to log scales. Figure \ref{fig:booksmodel} shows the empirical frequency in Google Books with a vertical logarithmic scale and the predictions of the model three centuries into the future after fitting it with the data since the year 2000. This suggest that approximately by the year 2256, all text in books will be composed of entirely ``Oh my God''. Our data-driven insights allow us to code a model that writes English fiction literature like the one we will see in the 23rd century:

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in}\NormalTok{ (}\KeywordTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{150}\NormalTok{)))}
\NormalTok{\{}
  \KeywordTok{cat}\NormalTok{(}\StringTok{"Oh my God"}\NormalTok{)}
\NormalTok{  rnd <-}\StringTok{ }\KeywordTok{runif}\NormalTok{(}\DataTypeTok{n=}\DecValTok{1}\NormalTok{, }\DataTypeTok{min=}\DecValTok{0}\NormalTok{, }\DataTypeTok{max=}\DecValTok{1}\NormalTok{)}
  \ControlFlowTok{if}\NormalTok{ (rnd }\OperatorTok{<}\FloatTok{0.4}\NormalTok{)}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"! "}\NormalTok{)}
  \ControlFlowTok{if}\NormalTok{ ((rnd }\OperatorTok{>=}\FloatTok{0.4}\NormalTok{) }\OperatorTok{&}\StringTok{ }\NormalTok{(rnd }\OperatorTok{<}\FloatTok{0.8}\NormalTok{))}
    \KeywordTok{cat}\NormalTok{(}\StringTok{". "}\NormalTok{)}
  \ControlFlowTok{if}\NormalTok{ (rnd }\OperatorTok{>=}\FloatTok{0.8}\NormalTok{)}
    \KeywordTok{cat}\NormalTok{(}\StringTok{"? "}\NormalTok{)}
\NormalTok{  \}}
\end{Highlighting}
\end{Shaded}

An example of the output of the model can be found here:

{
Oh my God! Oh my God. Oh my God! Oh my God? Oh my God! Oh my God. Oh my God. Oh my God! Oh my God? Oh my God. Oh my God? Oh my God. Oh my God! Oh my God. Oh my God. Oh my God? Oh my God! Oh my God! Oh my God. Oh my God? Oh my God. Oh my God! Oh my God. Oh my God! Oh my God! Oh my God. Oh my God! Oh my God. Oh my God. Oh my God. Oh my God! Oh my God. Oh my God! Oh my God? Oh my God! Oh my God. Oh my God. Oh my God! Oh my God? Oh my God. Oh my God? Oh my God. Oh my God! Oh my God. Oh my God. Oh my God? Oh my God! Oh my God! Oh my God. Oh my God? Oh my God. Oh my God! Oh my God. Oh my God! Oh my God! Oh my God. Oh my God! Oh my God. Oh my God. Oh my God. Oh my God! Oh my God. Oh my God! Oh my God? Oh my God! Oh my God. Oh my God. Oh my God! Oh my God? Oh my God. Oh my God? Oh my God. Oh my God! Oh my God. Oh my God. Oh my God? Oh my God! Oh my God! Oh my God. Oh my God? Oh my God. Oh my God! Oh my God. Oh my God! Oh my God! Oh my God. Oh my God! Oh my God. Oh my God. Oh my God. Oh my God! Oh my God. Oh my God! Oh my God? Oh my God! Oh my God. Oh my God. Oh my God! Oh my God? Oh my God. Oh my God? Oh my God. Oh my God! Oh my God. Oh my God. Oh my God? Oh my God! Oh my God! Oh my God. Oh my God? Oh my God. Oh my God! Oh my God. Oh my God! Oh my God! Oh my God. Oh my God! Oh my God. Oh my God. Oh my God. Oh my God! Oh my God. Oh my God! Oh my God? Oh my God! Oh my God. Oh my God. Oh my God! Oh my God? Oh my God. Oh my God? Oh my God. Oh my God! Oh my God. Oh my God. Oh my God? Oh my God! Oh my God! Oh my God. Oh my God? Oh my God. Oh my God! Oh my God. Oh my God! Oh my God! Oh my God. Oh my God! Oh my God. Oh my God. Oh my God.
}

\newpage


\hypertarget{the-one-with-the-other-languages}{%
\section{The one with the other languages}\label{the-one-with-the-other-languages}}

Given the international success of Friends, could this be happening in other languages too? We use the fact that Google Books covers eight languages to explore this possibility. We tried to find the closest equivalents of ``my God'', dropping the ``oh'' for better comparability across languages: ``Dios mío'' in Spanish, ``mio Dio'' in Italian, ``mon Dieu'' in French, and ``mein Gott'' in German. Adviced by native-speaking friends of us, we also included Chinese, Russian, and Hebrew versions, even though none of the authors speak these three languages\footnote{We thank Simon Schweighofer, Wenjuan Liang, Olga Antsiferova, and Amit Goldenberg for helping with our language handicaps}.

\begin{figure}

{\centering \includegraphics{Friends_HPS_pdf_files/figure-latex/booksplots-1} 

}

\caption{Frequency of terms equivalent to "my God" in the eight languages of Google Books.}\label{fig:booksplots}
\end{figure}

Figure \ref{fig:booksplots} shows the frequency of the equivalent of ``my God'' in English and the other seven languages. All languages except Chinese and Hebrew show sharp increases around 2008, coinciding with the growth of social media as a major communication mechanism. The pattern for English is softer, with a tendency to grow dating smoothly few decades back, and Russian also shows high historical frequency, especially around World War II. A common pattern in all languages is that their historical peak is in the last decade, showing that the overall tendency to use the phrase is growing beyond the English-speaking world.

\hypertarget{the-one-with-the-tweets}{%
\section{The one with the tweets}\label{the-one-with-the-tweets}}

How is ``oh my God'' being used in social media nowadays? To explore this, we used our \$14000/year subscription to Crimson Hexagon\footnote{Gently payed by other serious research projects}. We analyzed tweets from
2010-09-01 to 2020-08-30 that were identified by Crimson Hexagon as written in English by users living in the US and with an identifiable gender. Our analysis includes a total of 180300519164 tweets with gender and among them 130796995 contain ``oh my God'' or ``OMG''.

\begin{figure}

{\centering \includegraphics{Friends_HPS_pdf_files/figure-latex/twittergender-1} 

}

\caption{Donut plots of gender in tweets that contain OMG and in all tweets.}\label{fig:twittergender}
\end{figure}

Figure \ref{fig:twittergender} shows the fractions of tweets posted by users of each gender for tweets that include a form of OMG and for all tweets. 67.24\% of OMG tweets are posted by women, while 52.26\% of tweets in general are posted by women. This could mean that women use the phrase more often than men on Twitter or that the term is used as a predictor for gender by Crimson Hexagon. Distinguishing this would require other gender identification methods or self-reported data of Twitter users.

With respect to location, we can investigate the use of the phrase in the Los Angeles area by analyzing geolocated tweets:

\begin{figure}

{\centering \includegraphics[width=0.75\linewidth]{LAtweets} 

}

\caption{Heatmap of OMG tweets in the Los Angeles area.}\label{fig:twitterLA}
\end{figure}

The highest density of OMG tweets in Los Angeles is located around the areas of Santa Monica, Beverly Hills, and Hollywood. The San Fernando valley (top left) has also considerable density of OMG tweets, especially with respect to its overall lower tweet activity compared to the other regions. This shows how ``Oh my God'' is part of valleyspeak and in combination with the gender pattern, it explains the origin of the \emph{valley girl} stereotype.


We can also use location information to see the regional distribution of OMG across the United States. Figure \ref{fig:twitterstates} shows the US colored by the frequency of OMG per million tweets. One can see a higher frequency of OMG in the West coast and the northeast, with a lower frequency along the Bible belt. The top states by OMG frequency are Oregon and Washington, with California surprisingly low in 9th position.


\begin{figure}

{\centering \includegraphics[width=\linewidth]{Friends_HPS_pdf_files/figure-latex/twitterstates-1} 

}

\caption{Map of the United States with states colored according to their frequency of OMG per million tweets.}\label{fig:twitterstates}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{|c|l|r||c|l|r|}
\hline
position & state & perMillion &  position & state & perMillion\\
\hline
\hline
1 & Oregon & 746.5444 & 41 & Delaware & 506.6569\\
\hline
2 & Washington & 734.2142 & 42 & Montana & 457.7706\\
\hline
3 & Massachusetts & 724.0687 & 43 & Wyoming & 457.6181\\
\hline
4 & Maine & 723.2998 & 44 & North Carolina & 454.8653\\
\hline
5 & Minnesota & 706.8594 & 45 & Alabama & 435.6764\\
\hline
6 & New Hampshire & 706.3069 & 46 & Georgia & 428.4706\\
\hline
7 & Wisconsin & 688.0421 & 47 & South Carolina & 426.1360\\
\hline
8 & Arizona & 677.9887 & 48 & Arkansas & 424.6931\\
\hline
9 & California & 677.7345 & 49 & Louisiana & 400.4717\\
\hline
10 & Hawaii & 672.7159 & 50 & Mississippi & 337.1873\\
\hline
\end{tabular}
\caption{Top and bottom 10 states by frequency of OMG in tweets.}
\end{table}



\newpage

The bottom states by frequency of OMG in tweets are southern states like Mississippi and Louisiana. Since ``Oh my God'' can be considered blasphemy under traditional Christian values, one can expect the frequency of this expression to be lower in more conservative states. To test this idea, we collected the US 2020 presidential election results from \url{https://cookpolitical.com/2020-national-popular-vote-tracker}.


\begin{figure}

{\centering \includegraphics[width=0.7\textwidth]{Friends_HPS_pdf_files/figure-latex/twittertrump-1} 

}

\caption{Scatter plot of Donald Trump vote percentage in 2020 versus OMG per million tweets across US states. The line and shaded areas show the results of a linear fit.}\label{fig:twittertrump}
\end{figure}

Figure \ref{fig:twittertrump} shows the association between the percentage of votes for Trump in 2002 and the frequency of OMG per million tweets.
Over states, the percentage of votes to Donald Trump is negatively correlated with the frequency of OMG tweets (Pearson's correlation coefficent of -0.47 with p-value 0.0006). This significant correlation satisfies the necessary condition for any paper or blog post to refer to Donald Trump or to COVID-19 to get any attention on Twitter.

\newpage

\hypertarget{the-one-with-artificial-intelligence}{%
\section{The one with Artificial Intelligence}\label{the-one-with-artificial-intelligence}}

To understand better the meaning of ``Oh my God'', we use everyone's favorite technology nowadays: \emph{``Artificial Intelligence''}. More precisely, we inspect three deep language models trained against large-scale data: BERT (Devlin et al. \protect\hyperlink{ref-devlin2018bert}{2018}), GPT-2 (Radford et al. \protect\hyperlink{ref-radford2019language}{2019}), and RoBERTa (Liu et al. \protect\hyperlink{ref-liu2019roberta}{2019}). We do this through the models available in \href{https://huggingface.co/}{Huggingface} and the \href{https://pypi.org/project/transformers/}{transformers python package}.

BERT allows us to predict a word based on the context in which it appears. We calculated the distribution of word probabilities to fill the sentence "Oh my \_\_\_" to see if God seems to be the most frequent word. The top 10 words by probability are shown below:

\begin{table}[h]
\centering
\begin{tabular}{|l|r|}
\hline
word & score\\
\hline
god & 0.9291785\\
\hline
goodness & 0.0245932\\
\hline
gods & 0.0092625\\
\hline
lord & 0.0071211\\
\hline
goddess & 0.0059075\\
\hline
word & 0.0016963\\
\hline
zeus & 0.0013340\\
\hline
go & 0.0009128\\
\hline
my & 0.0008139\\
\hline
head & 0.0007671\\
\hline
\end{tabular}
\caption{Top words with BERT scores for presence after "Oh my".}
\end{table}

The probablity of ``god'' is by far the highest, close to 0.93. This is followed by ``goodness'', which fits as a similar exclamation but without having religious connotations. Beyond that, all probabilities are below 0.01, showing how prevalent is the word ``god'' in this context.

With GPT-2, we can generate a longer text following ``Oh my God!'' to have an idea of the context of the phrase. First, we produce the most likely text following the sentence:

{
Oh my God! I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry.
}

Interestingly, the model gets stuck in a loop of ``I'm so sorry''. We have run the generation up to very long texts and the result was the same, an endless stream of ``I'm so sorry'' is the most likely text to follow ``Oh my God!'' in GPT-2.

We generated 10.000 random texts with GPT-2 to have an idea of the contexts that can follow ``Oh my God!''. Below we show the first four as examples:

{[}1{]} ``\emph{Oh my God! ~Seriously, that's unbelievable. And to think that we are so ignorant as to not care about}''

{[}1{]} ``\emph{Oh my God! ~I just can't believe ~that this is that. ~I mean ~I}''

{[}1{]} ``\emph{Oh my God! ~I'm going to be dead now ~and we're going to be dead for years to}''

{[}1{]} ``\emph{Oh my God! ~It is so incredibly cute~and very, very ~hot! ~You know,}''

\newpage

We see that the phrase can lead to both negative statements, for example the first, but also to postive statements, like the last one. A common factor is the general expression of surprise and of first person singular sentences that highlight the expression of emotions following the sentence. This becomes clearer if we generate a word cloud\footnote{Just for illustration purposes} of the generated texts:

%\vspace{1cm}

\begin{center}\includegraphics{Friends_HPS_files/figure-html/unnamed-chunk-14-1.png} \end{center}

%\vspace{1cm}


The word ``I'' is by far the most frequent, followed by verbs and that express personal experiences and feelings. The word ``like'' is also visible, highlighting this way another of the language patterns of \emph{valleyspeak}.

\newpage

To have an idea of the affective meanings of the phrase, we use a \href{https://huggingface.co/textattack/roberta-base-MNLI}{RoBERTa model tuned for NLI}. We set up a zero-shot classification task as a Natural Language Inference task that classifies the phrase ``Oh my God!'' using the hypothesis ``This sentence expresses \{\}.'', where the brackets are filled with a single word that will serve as the candidate label. As candidate labels, we use the \href{https://www.unige.ch/cisa/research/topics/specific-research-projects/language-and-culture/grid-project/emotion-words/}{24 emotion words of the GRID project} (Fontaine, Scherer, and Soriano \protect\hyperlink{ref-fontaine2013components}{2013}). The words with the highest score in this task are the following:


\begin{table}[h]
\centering
\begin{tabular}{|l|r|}
\hline
word & score\\
\hline
surprise & 0.0982359\\
\hline
disappointment & 0.0909134\\
\hline
disgust & 0.0818704\\
\hline
irritation & 0.0763046\\
\hline
interest & 0.0741176\\
\hline
despair & 0.0681486\\
\hline
being hurt & 0.0652222\\
\hline
sadness & 0.0578116\\
\hline
anger & 0.0518757\\
\hline
contempt & 0.0506069\\
\hline
\end{tabular}
\caption{Top words by score in the zero-shot emotion classification task.}
\end{table}

As expected, ``surprise'' is the closest emotion, followed by a series of negative emotions including ``disappointment'' and ``disgust''. In the top 10, the only two non-negative emotions are ``surprise'' and ``interest'', suggesting that, in terms of emotion classification, the phrase ``Oh my God'' is first related to surprise and then to negative emotions, with positive emotions being lower in the ranking.

\hypertarget{the-one-with-the-conclusions}{%
\section{The one with the conclusions}\label{the-one-with-the-conclusions}}

To summarize, we have found the following:

\begin{itemize}
\item
  The frequency of OMG per episode in Friends increased over its run, as it did in contemporary TV and movies. However, Friends had more than four times the frequency of the phrase as other TV and movie content.
\item
  The frequency of OMG in books was exponentially increasing before Friends was released. This growth rate accelerated after Friends became popular.
\item
  This growth pattern appears in othe languages too, having a pronounced acceleration at the beggining of the 2010s decade.
\item
  Tweets that contain OMG in the US are more likely to be posted by women and regions with higher frequency of OMG tweets tend to have lower vote shares for Donald Trump in the 2020 election.
\item
  ``God'' is the word that fits best after ``Oh my'' and the text following the phrase can be both positive and negative, but in general subjective.
\item
  Large language models associate the phrase with surprise and then with negative emotions.
\end{itemize}

\vspace{1cm}
{
\Large
Thanks for reading and all the best for 2021!
}
\newpage

\hypertarget{references}{%
\section*{References}\label{references}}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\leavevmode\hypertarget{ref-Albright2015}{}%
Albright, Alex. 2015. ``The One with All the Quantifiable Friendships the Little Dataset.'' \emph{The Little Dataset That Could}. \url{https://thelittledataset.com/2015/01/20/the-one-with-all-the-quantifiable-friendships/}.

\leavevmode\hypertarget{ref-bhattacharyya_sentiment_2019}{}%
Bhattacharyya, Shilpi. 2019. ``Sentiment Analysis of the Lead Characters on F.R.I.E.N.D.S.'' \emph{Medium}. \url{https://towardsdatascience.com/sentiment-analysis-of-the-lead-characters-on-f-r-i-e-n-d-s-51aa5abf1fa6}.

\leavevmode\hypertarget{ref-ngramr}{}%
Carmody, Sean. 2020. \emph{Ngramr: Retrieve and Plot Google N-Gram Data}. \url{https://github.com/seancarmody/ngramr}.

\leavevmode\hypertarget{ref-davies2010corpus}{}%
Davies, Mark. 2010. ``The Corpus of Contemporary American English as the First Reliable Monitor Corpus of English.'' \emph{Literary and Linguistic Computing} 25 (4). Oxford University Press: 447--64.

\leavevmode\hypertarget{ref-devlin2018bert}{}%
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. ``Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding.'' \emph{arXiv Preprint arXiv:1810.04805}.

\leavevmode\hypertarget{ref-tm1}{}%
Feinerer, Ingo, and Kurt Hornik. 2020. \emph{Tm: Text Mining Package}. \url{https://CRAN.R-project.org/package=tm}.

\leavevmode\hypertarget{ref-tm2}{}%
Feinerer, Ingo, Kurt Hornik, and David Meyer. 2008. ``Text Mining Infrastructure in R.'' \emph{Journal of Statistical Software} 25 (5): 1--54. \url{https://www.jstatsoft.org/v25/i05/}.

\leavevmode\hypertarget{ref-fontaine2013components}{}%
Fontaine, Johnny RJ, Klaus R Scherer, and Cristina Soriano. 2013. \emph{Components of Emotional Meaning: A Sourcebook}. Oxford University Press.

\leavevmode\hypertarget{ref-forecast1}{}%
Hyndman, Rob, George Athanasopoulos, Christoph Bergmeir, Gabriel Caceres, Leanne Chhay, Mitchell O'Hara-Wild, Fotios Petropoulos, Slava Razbash, Earo Wang, and Farah Yasmeen. 2020. \emph{forecast: Forecasting Functions for Time Series and Linear Models}. \url{https://pkg.robjhyndman.com/forecast/}.

\leavevmode\hypertarget{ref-forecast2}{}%
Hyndman, Rob J, and Yeasmin Khandakar. 2008. ``Automatic Time Series Forecasting: The Forecast Package for R.'' \emph{Journal of Statistical Software} 26 (3): 1--22. \url{https://www.jstatsoft.org/article/view/v027i03}.

\leavevmode\hypertarget{ref-liu2019roberta}{}%
Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. ``Roberta: A Robustly Optimized Bert Pretraining Approach.'' \emph{arXiv Preprint arXiv:1907.11692}.

\leavevmode\hypertarget{ref-loscalzo_2018}{}%
Loscalzo, Michael. 2018. ``Oh, My God!'' \emph{Medium}. \url{https://towardsdatascience.com/oh-my-god-cb69dd74839c}.

\leavevmode\hypertarget{ref-michel2011quantitative}{}%
Michel, Jean-Baptiste, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K Gray, Joseph P Pickett, Dale Hoiberg, et al. 2011. ``Quantitative Analysis of Culture Using Millions of Digitized Books.'' \emph{Science} 331 (6014). American Association for the Advancement of Science: 176--82.

\leavevmode\hypertarget{ref-pechenick2015characterizing}{}%
Pechenick, Eitan Adam, Christopher M Danforth, and Peter Sheridan Dodds. 2015. ``Characterizing the Google Books Corpus: Strong Limits to Inferences of Socio-Cultural and Linguistic Evolution.'' \emph{PloS One} 10 (10). Public Library of Science: e0137041.

\leavevmode\hypertarget{ref-radford2019language}{}%
Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. ``Language Models Are Unsupervised Multitask Learners.'' \emph{OpenAI Blog} 1 (8): 9.

\leavevmode\hypertarget{ref-sahakyan_one_2019}{}%
Sahakyan, Elizabeth Ter. 2019. ``The One with the Data Scientist: A Closer Look at the Friends of Friends.'' \emph{Medium}. \url{https://medium.com/@liztersahakyan/the-one-with-the-data-scientist-a-closer-look-at-the-friends-of-friends-d3530d1902af}.

\leavevmode\hypertarget{ref-sohoye_one_2019}{}%
Sohoye, Yusuf. 2019. ``The One with All the FRIENDS Analysis.'' \emph{Medium}. \url{https://towardsdatascience.com/the-one-with-all-the-friends-analysis-59dafcec19c5}.

\leavevmode\hypertarget{ref-TheEconomist_2019}{}%
The Economist. 2019. ``Why `Friends' Is Still the World's Favourite Sitcom, 25 Years on,'' September. \url{https://www.economist.com/prospero/2019/09/20/why-friends-is-still-the-worlds-favourite-sitcom-25-years-on}.

\leavevmode\hypertarget{ref-noauthor_valleyspeak_2020}{}%
Wikipedia. 2020. ``Valleyspeak.'' \url{https://en.wikipedia.org/w/index.php?title=Valleyspeak\&oldid=992610782}.

\end{document}
